{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59ad4fd",
   "metadata": {},
   "source": [
    "# Summary\n",
    "## 1. Initial Model Training: Train an initial CNN model using your labeled dataset.\n",
    "## 2. Continuous Data Ingestion: Collect new labeled data using Power Apps or another tool.\n",
    "## 3. Self-Learning Loop: Periodically retrain the model with the new data and save the updated model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9daff4",
   "metadata": {},
   "source": [
    "Specific Recommendations\n",
    "\n",
    "# Image Classification:\n",
    "- From Scratch: Typically, 1,000+ images per class.\n",
    "- Transfer Learning: 100-1,000 images per class can suffice, depending on the task complexity and similarity to the pre-trained model's domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24fc97c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 17:02:01.245376: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'file_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Execute initial training\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m initial_training()\n",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m, in \u001b[0;36minitial_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitial_training\u001b[39m():\n\u001b[0;32m---> 70\u001b[0m     df \u001b[38;5;241m=\u001b[39m read_excel(excel_path)\n\u001b[1;32m     71\u001b[0m     train_df, val_df \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     72\u001b[0m     train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(excel_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_excel\u001b[39m(excel_path):\n\u001b[1;32m     21\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(excel_path)\n\u001b[0;32m---> 22\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, x))\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file_name'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Define directories and parameters\n",
    "excel_path = \"/Users/colleenjung/Downloads/Book.xlsx\"  # Path to the Excel file\n",
    "image_dir = \"/Users/colleenjung/Desktop/UChicago/24SummerCorrugated/Transit Damage\"  # Replace with the directory where images are stored\n",
    "image_size = (224, 224)\n",
    "model_path = 'damage_classification_model.h5'  # Path to save the model\n",
    "batch_size = 32\n",
    "\n",
    "# Function to read Excel file and preprocess data\n",
    "def read_excel(excel_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    df['file_path'] = df['file_name'].apply(lambda x: os.path.join(image_dir, x))\n",
    "    return df\n",
    "\n",
    "# Custom data generator\n",
    "def custom_data_generator(df, batch_size, image_size, subset):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    if subset == 'training':\n",
    "        subset_df = df[df['subset'] == 'training']\n",
    "    else:\n",
    "        subset_df = df[df['subset'] == 'validation']\n",
    "    \n",
    "    while True:\n",
    "        for start in range(0, len(subset_df), batch_size):\n",
    "            end = min(start + batch_size, len(subset_df))\n",
    "            batch_df = subset_df[start:end]\n",
    "            \n",
    "            images = []\n",
    "            labels = []\n",
    "            \n",
    "            for _, row in batch_df.iterrows():\n",
    "                img = load_img(row['file_path'], target_size=image_size)\n",
    "                img = img_to_array(img)\n",
    "                images.append(img)\n",
    "                labels.append(row['label'])\n",
    "            \n",
    "            images = np.array(images, dtype='float32') / 255.0\n",
    "            labels = to_categorical(labels, num_classes=df['label'].nunique())\n",
    "            \n",
    "            yield images, labels\n",
    "\n",
    "# Function to build and compile the model\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initial training of the model\n",
    "def initial_training():\n",
    "    df = read_excel(excel_path)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'])\n",
    "    train_df['subset'] = 'training'\n",
    "    val_df['subset'] = 'validation'\n",
    "    df = pd.concat([train_df, val_df])\n",
    "    \n",
    "    num_classes = df['label'].nunique()\n",
    "    \n",
    "    train_generator = custom_data_generator(df, batch_size, image_size, 'training')\n",
    "    validation_generator = custom_data_generator(df, batch_size, image_size, 'validation')\n",
    "    \n",
    "    model = build_model(num_classes)\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_df) // batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(val_df) // batch_size,\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved at {model_path}\")\n",
    "\n",
    "# Execute initial training\n",
    "initial_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69ac577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Main Product Type', 'Material Name', 'ID', 'Material Shape',\n",
      "       'Paper Type', 'Weight in Lbs. Per Sq Inch', 'Width in Inch',\n",
      "       'Length in Inch', 'Diameter in Inch', 'Core', 'Length in Feet',\n",
      "       'Weight in Lbs.', 'Moisture', 'Image #', 'Main Damage Reason', 'Notes',\n",
      "       'Source', 'Damage Dimensions', 'DamageLocation', 'Method', '#Layers'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The expected 'file_name' or 'filename' column is not found in the Excel file.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Execute initial training\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m initial_training()\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36minitial_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitial_training\u001b[39m():\n\u001b[0;32m---> 21\u001b[0m     df \u001b[38;5;241m=\u001b[39m read_excel(excel_path)\n\u001b[1;32m     22\u001b[0m     train_df, val_df \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m     train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(excel_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, x))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe expected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column is not found in the Excel file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The expected 'file_name' or 'filename' column is not found in the Excel file.\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_dir = \"/Users/colleenjung/Desktop/UChicago/24SummerCorrugated/Transit Damage\"\n",
    "excel_path = \"/Users/colleenjung/Downloads/Book.xlsx\"\n",
    "\n",
    "def read_excel(excel_path):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(df.columns)  # Print the columns to debug\n",
    "    # Use the correct column name\n",
    "    if 'file_name' in df.columns:\n",
    "        df['file_path'] = df['file_name'].apply(lambda x: os.path.join(image_dir, x))\n",
    "    elif 'filename' in df.columns:  # Example alternative column name\n",
    "        df['file_path'] = df['filename'].apply(lambda x: os.path.join(image_dir, x))\n",
    "    else:\n",
    "        raise KeyError(\"The expected 'file_name' or 'filename' column is not found in the Excel file.\")\n",
    "    return df\n",
    "\n",
    "def initial_training():\n",
    "    df = read_excel(excel_path)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'])\n",
    "    train_df['subset'] = 'training'\n",
    "    val_df['subset'] = 'validation'\n",
    "    # Your training code here...\n",
    "    print(\"Training completed.\")\n",
    "    model_path = \"path_to_save_model.h5\"  # Update this to your desired model save path\n",
    "    # model.save(model_path)\n",
    "    print(f\"Model saved at {model_path}\")\n",
    "\n",
    "# Execute initial training\n",
    "initial_training()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
